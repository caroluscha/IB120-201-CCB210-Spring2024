{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(ggplot2)\n",
    "library(cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"inputs/comparingClusters.png\" width=50%/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$c(A) = \\frac{1}{|A|}\\sum_{x \\in A} x$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distance between two clusters is:\n",
    "\n",
    "$$d(C_1, C_2) = d(c(C_1), c(C_2))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UPGMA - unweighted pair group method with arithmetic mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$d(C_1, C_2) = \\frac{1}{|C_1| \\times |C_2|}\\sum_{x \\in C_1}\\sum_{y \\in C_2}d(x,y)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm works as follows:\n",
    "\n",
    "* Put each data point in its own cluster.\n",
    "* Identify the closest two clusters and combine them into one cluster.\n",
    "* Repeat the above step till all the data points are in a single cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "levels(iris$Species)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head(iris[,3:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`hclust()` syntax: `hclust(data, method)`, where `data` is a dissimilarity structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = dist(iris[, 3:4], method = \"euclidean\")\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, for a data sample of size M, the distance matrix is an M × M symmetric matrix with M × (M - 1)∕2 distinct elements. Hence for a data sample of size 150 (the number of observations in `iris`), its distance matrix has about 11,000 distinct elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters1 <- hclust(d) #defaults to complete linkage method (not covered)\n",
    "clusters1 # object of class hclust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(clusters1)\n",
    "rect.hclust(clusters1 , k = 3, border = 2:6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterCut1 <- cutree(clusters1, 3)\n",
    "clusterCut1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table(clusterCut1, iris$Species)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ggplot(iris, aes(x=Petal.Length, y=Petal.Width, color=Species)) + geom_point(size = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question\n",
    "\n",
    "Let's see if using a different clustering method will give better results. Try the `average` method on your own (add the argument `method` to the `hclust` function). Plot the respective dendogram. Compare cluster membership among k=3 clusters and the members between species."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In k means clustering, we have to specify the number of clusters, $k$, we want the data to be grouped into. The algorithm randomly picks $k$ observations and assigns them each to their own cluster. These points are the centroids of their clusters. Then, the algorithm iterates through two steps:\n",
    "* Reassign data points to the cluster whose centroid is closest.\n",
    "* Calculate new centroid of each cluster.\n",
    "\n",
    "These two steps are repeated till the within cluster variation cannot be reduced any further. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`kmeans()` syntax: `kmeans(data, centers)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(20) #ensures everyone gets the same results\n",
    "kCluster <- kmeans(iris[, 3:4], 3, nstart = 20)\n",
    "kCluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kCluster$tot.withinss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table(kCluster$cluster, iris$Species)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the data belonging to the setosa species got grouped into cluster 3, versicolor into cluster 1, and virginica into cluster 2. The algorithm wrongly classified two data points belonging to versicolor and four data points belonging to virginica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot the data to see the clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kCluster$cluster <- as.factor(kCluster$cluster)\n",
    "ggplot(iris, aes(Petal.Length, Petal.Width, color = kCluster$cluster)) + geom_point(size = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ggplot(iris, aes(x=Petal.Length, y=Petal.Width, color=Species)) + geom_point(size = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ggplot(iris, aes(Petal.Length, Petal.Width, color = Species)) +\n",
    "    geom_point(alpha = 0.4, size = 3.5) + geom_point(col = kCluster$cluster) +\n",
    "    scale_color_manual(values = c('green', 'black', 'red'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determining optimal number of clusters (k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Elbow Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$TWCSS = \\sum_{i=1}^k\\sum_{x \\in C_i}(x-\\mu_i)^2$$\n",
    "$$\\mu_i = c(C_i)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Elbow Method for finding the optimal number of clusters\n",
    "# Compute and plot wss for k = 1 to k = 7.\n",
    "k.max <- 7 #max 150 (num of obs)\n",
    "twcss <- sapply(1:k.max, \n",
    "              function(k){kmeans(iris[, 3:4], k, nstart=20)$tot.withinss})\n",
    "\n",
    "plot(1:k.max, twcss,\n",
    "     type=\"b\", pch = 19, frame = FALSE, \n",
    "     xlab=\"Number of clusters K\",\n",
    "     ylab=\"Total within-clusters sum of squares\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Silhouette Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$a(x) = \\frac{1}{|C_i|-1}\\sum_{y \\in C_i, x \\neq y}d(x,y)$$\n",
    "$$b(x) = min_{k \\neq i}\\big(\\frac{1}{|C_i|}\\sum_{y \\in C_k}d(x,y)\\big)$$\n",
    "$$s(x) = \\begin{cases} \\frac{b(x)-a(x)}{max(a(x),b(x))}, & \\text{if $|C_i| > 1$}.\\\\ 0, & \\text{if $|C_i| = 1$}. \\end{cases}$$\n",
    "\n",
    "The silhouette *score* is the $mean(s(x))$ for all $x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=3\n",
    "km <- kmeans(iris[, 3:4], centers = k, nstart=20)\n",
    "ss <- silhouette(km$cluster, dist(iris[, 3:4], \"euclidean\"))\n",
    "ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#silhouette method\n",
    "avg_silhouette_score <- function(k){\n",
    "  km <- kmeans(iris[, 3:4], centers = k, nstart=20)\n",
    "  ss <- silhouette(km$cluster, dist(iris[, 3:4], \"euclidean\"))\n",
    "  mean(ss[, 3])\n",
    "}\n",
    "k <- 2:7 #minimum number of clusters for silhouette scores is 2 for between-cluster variation\n",
    "avg_sil <- sapply(k, avg_silhouette_score)\n",
    "k <- c(0, k)\n",
    "avg_sil <- c(0, avg_sil)\n",
    "plot(k, avg_sil, type='b', , xlab='Number of clusters', ylab='Average Silhouette Scores', frame=FALSE, ylim = c(0,0.8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question\n",
    "\n",
    "Run a K-means analysis for 2 clusters. Compare the table for membership within clusters and membership within species for this analysis to the table from the k=3 analysis. Do you notice anything interesting? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Food for thought: what biological reason might there be that versicolor and virginica are getting clustered together?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(ggplot2)\n",
    "library(cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Read in the hw12.csv and make a new Data.Frame out of the columns Annual Income & Spending Score (1-100).\n",
    "\n",
    "    (A) Perform centroid hierarchical clustering on the dataset and visualize it in a dendogram.\n",
    "\n",
    "    (B) Perform UPGMA hierarchical clustering on the dataset and visualize it in a dendogram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Choose one of the above methods. Cut the dendogram using cutree into k=2 clusters. Create a table that compares the clustering to the gender of each customer (refer back to the original data). Is there reason to believe the data is dependent on gender? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Perform a K-means analysis, from 1 to 10 clusters. Create an elbow plot. Which value does the plot suggest is the optimal number of clusters?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Perform a K-means analysis, up to 10 clusters. Create a silhouette plot (recall the minimum number of clusters needed for this kind of plot). Does this value agree with the value from question 3?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
